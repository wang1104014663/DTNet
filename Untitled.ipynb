{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-29T19:24:04.425127Z",
     "start_time": "2018-05-29T19:24:03.399011Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import math\n",
    "import argparse\n",
    "import random\n",
    "import os\n",
    "from my_net import AttributeNetwork\n",
    "#from my_net import RelationNetwork\n",
    "from my_net import TripletNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-29T19:24:04.429208Z",
     "start_time": "2018-05-29T19:24:04.425218Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPISODE = 500000\n",
    "TEST_EPISODE = 1000\n",
    "LEARNING_RATE =1e-5\n",
    "GPU = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CUBTriplets(data.Dataset):\n",
    "    def __init__(self, dateset='AwA1', n_triplets=10000, classes=range(50)):\n",
    "\n",
    "        # paths\n",
    "        self.root = './data/'\n",
    "        self.im_base_path = os.path.join(root,datset)\n",
    "        image_embedding = 'res101'               #ResNet101层\n",
    "        class_embedding = 'original_att'         #属性表达 85-d\n",
    "\n",
    "        # load feture \n",
    "        matcontent = sio.loadmat(dataroot + \"/\" + dataset + \"/\" + image_embedding + \".mat\")  #scipy loadmat\n",
    "        \n",
    "        feature = matcontent['features'].T\n",
    "        \n",
    "        labels = matcontent['labels'].astype(int).squeeze() - 1   #matlab begin 1 ,numpy begin 0\n",
    "        \n",
    "        matcontent = sio.loadmat(dataroot + \"/\" + dataset + \"/\" + class_embedding + \"_splits.mat\")\n",
    "        \n",
    "        trainval_loc = matcontent['trainval_loc'].squeeze() - 1    #squeeze()去掉维度中的1 AxBx1 --->AxB\n",
    "        \n",
    "        #test_seen_loc = matcontent['test_seen_loc'].squeeze() - 1\n",
    "        #test_unseen_loc = matcontent['test_unseen_loc'].squeeze() - 1        \n",
    "        \n",
    "        #load attr\n",
    "        attribute = matcontent['att'].T                #转置 50x85 每行是整个属性向量\n",
    "        \n",
    "        x = feature[trainval_loc]                      # train_features trainval里面是图片的编号 begin with 0 ，19832个\n",
    "       \n",
    "        train_label = label[trainval_loc].astype(int)  # train_label  int类型没变 每个图片的lable 19832个\n",
    "        \n",
    "        train_id = np.unique(train_label)                     # train_id  40个类 ，unique去重\n",
    "        \n",
    "        att = attribute[train_label]                   # train attributes 每个图片的属性 19832个\n",
    "        \n",
    "        x_test = feature[test_unseen_loc]                   # test_feature 5685个\n",
    "        test_label = label[test_unseen_loc].astype(int)     # test_label   5685个\n",
    "    \n",
    "        x_test_seen = feature[test_seen_loc]                #test_seen_feature 4958个\n",
    "        test_label_seen = label[test_seen_loc].astype(int)  # test_seen_label  4958个\n",
    "        \n",
    "        test_id = np.unique(test_label)                     # test_id  10个类 ，unique去重\n",
    "        att_pro = attribute[test_id]                        # test_attribute 每一类的属性向量 10x85\n",
    "\n",
    "        train_features = torch.from_numpy(x)   #np-->tensor\n",
    "\n",
    "        sample_attributes=[]\n",
    "        train_label = torch.from_numpy(train_label).unsqueeze(1) #每张图片的属性转化 ，unsqueeze(1)就是插入到第一维度 AxB维-->Ax1xB\n",
    "\n",
    "        all_attributes = np.array(attribute)  #所有50类属性转变为numpy数组???属性向量仍然用的numpy类型 没有转化为pytorch\n",
    "\n",
    "        attributes = torch.from_numpy(attribute) \n",
    "\n",
    "        test_features = torch.from_numpy(x_test)\n",
    "\n",
    "        test_label = torch.from_numpy(test_label).unsqueeze(1)\n",
    "\n",
    "        testclasses_id = np.array(test_id)\n",
    "\n",
    "        test_attributes = torch.from_numpy(att_pro).float()\n",
    "\n",
    "        test_seen_features = torch.from_numpy(x_test_seen)\n",
    "\n",
    "        test_seen_label = torch.from_numpy(test_label_seen)\n",
    "\n",
    "        #train_data = TensorDataset(train_features,train_label)\n",
    "        \n",
    "        #name_to_id = dict(zip(birdnames, range(len(birdnames))))  \n",
    "#zip() 函数用于将可迭代的对象作为参数，将对象中对应的元素打包成一个个元组，然后返回由这些元组组成的列表。\n",
    "#如果各个迭代器的元素个数不一致，则返回列表长度与最短的对象相同，利用 * 号操作符，可以将元组解压为列表。\n",
    "\n",
    "        # which classes to include\n",
    "        #self.classes = classes\n",
    "        #self.num_classes = len(classes)\n",
    "        self.classes = train_id\n",
    "        self.num_classes = len(train_id)\n",
    "\n",
    "        # load list and metadata for train/test set\n",
    "        # paths\n",
    "        self.images = x #numpy 类型 \n",
    "        # labels\n",
    "        self.labels = train_label \n",
    "\n",
    "        # make triplets\n",
    "        self.num_triplets = n_triplets\n",
    "        self.make_triplet_list(n_triplets)\n",
    "        \n",
    "\n",
    "        print(\"CUB triplet loader initialized for %d classes, %d triplets\" % (self.num_classes, n_triplets))\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        idx1, idx2 = self.triplets[index]\n",
    "        img1 = self.images[idx1]\n",
    "        img2 = self.images[idx2]\n",
    "        \n",
    "        \n",
    "        return img1, img2, idx1, idx2\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_triplets\n",
    "\n",
    "    def make_triplet_list(self, ntriplets):\n",
    "        print('Processing Triplet Generation ...')\n",
    "        self.triplets = []\n",
    "        nc = int(self.num_classes)\n",
    "        for cx in range(nc):\n",
    "            class_idx = self.classes[cx]\n",
    "            # a, b, c are index of labels where it's equal to class_idx\n",
    "            a = np.random.choice(np.where(self.labels==class_idx)[0],\n",
    "                                 int(ntriplets/nc), replace=True)    \n",
    "            '''Return elements, either from x or y, depending on condition.\n",
    "\n",
    "                If only condition is given, return condition.nonzero().\n",
    "\n",
    "                    Parameters:\n",
    "                    condition : array_like, bool\n",
    "                    When True, yield x, otherwise yield y.'''\n",
    "            #b = np.random.choice(np.where(self.labels==class_idx)[0],\n",
    "            #                     int(ntriplets/nc), replace=True)\n",
    "            #while np.any((a-b)==0): #aligning check\n",
    "            #    np.random.shuffle(b)\n",
    "            b = np.random.choice(np.where(self.labels!=class_idx)[0],\n",
    "                                 int(ntriplets/nc), replace=True)\n",
    "\n",
    "            #for i in range(a.shape[0]):\n",
    "            #    self.triplets.append((a[i], b[i], c[i]))\n",
    "            self.triplets += zip(a,b)\n",
    "        np.random.shuffle(self.triplets)\n",
    "\n",
    "        print('Done!')\n",
    "#######################################################################################################################################\n",
    "#######################################################################################################################################\n",
    "#######################################################################################################################################\n",
    "    def regenerate_triplet_list(self, sampler, frac_hard):\n",
    "        print(\"Processing Triplet Regeneration ...\")\n",
    "        # negatives is a tuple of anchors and negative examples\n",
    "        num_random_triplets = self.num_triplets*(1.0-frac_hard)\n",
    "        # adjust number of random triplets so that it is a multiple of num_classes\n",
    "        num_random_triplets = int(math.ceil(num_random_triplets)/self.num_classes)*self.num_classes\n",
    "        num_hard = self.num_triplets - num_random_triplets\n",
    "        print(\"Number of hard triplets %d ...\" % num_hard)\n",
    "        self.make_triplet_list(num_random_triplets)\n",
    "        neg_hard_examples = sampler.ChooseNegatives(num_hard)\n",
    "        # choose random positives (for now atleast) for hard negatives\n",
    "        for pair in neg_hard_examples:\n",
    "            a, c = pair\n",
    "            anchor_class = self.labels[a]\n",
    "            b = np.random.choice(np.where(self.labels == anchor_class)[0])\n",
    "            self.triplets.append((a, b, c))\n",
    "        np.random.shuffle(self.triplets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init dataset\n",
      "torch.Size([19832, 2048])\n",
      "torch.Size([19832, 1])\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-29T22:31:06.174481Z",
     "start_time": "2018-05-29T22:31:05.101027Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "# step 1: init dataset\n",
    "    print(\"init dataset\")\n",
    "##################################参数##################################################################\n",
    "    dataroot = './data'\n",
    "    dataset = 'AwA1_data'\n",
    "    image_embedding = 'res101'               #ResNet101层\n",
    "    class_embedding = 'original_att'         #属性表达 85-d\n",
    "#######################################读取视觉特征###################################################################\n",
    "\n",
    "    matcontent = sio.loadmat(dataroot + \"/\" + dataset + \"/\" + image_embedding + \".mat\")  #scipy loadmat\n",
    "    #print(matcontent)\n",
    "    #print(matcontent.keys())           #featrues 2048x30478 double ,image_files 30475x1 cell,lables 30475x1 double\n",
    "                                        #30475个picture 2048是视觉特征\n",
    "    #print(matcontent['__header__'])    #matlab文件信息\n",
    "    #print(matcontent['__globals__'])\n",
    "    #print(matcontent['__version__'])\n",
    "    #print(matcontent['features'],matcontent['features'].shape)\n",
    "    \n",
    "    feature = matcontent['features'].T         #转置 30478x2048 每一行是一个完整的样本\n",
    "    #print(feature,feature.shape)\n",
    "    #print(matcontent['labels'].astype(int).dtype)  #uint8--->int32 mat使用numpy读取存在存储类型的差别\n",
    "    label = matcontent['labels'].astype(int).squeeze() - 1   #matlab begin 1 ,numpy begin 0\n",
    "########################################读取属性特征###########################################################\n",
    "\n",
    "    matcontent = sio.loadmat(dataroot + \"/\" + dataset + \"/\" + class_embedding + \"_splits.mat\")\n",
    "    #print(matcontent.keys())\n",
    "    #'allclasses_names' 50x1 cell 所有类名 ，'att',所有类的属性值 50x85double \n",
    "    #'test_seen_loc','test_unseen_loc','trainval_loc','val_loc','train_loc' 数据集分割\n",
    "    \n",
    "    # numpy array index starts from 0, matlab starts from 1\n",
    "    trainval_loc = matcontent['trainval_loc'].squeeze() - 1    #squeeze()去掉维度中的1 AxBx1 --->AxB\n",
    "    #print(matcontent['trainval_loc'],matcontent['trainval_loc'].shape)\n",
    "    #print(trainval_loc,trainval_loc.shape)\n",
    "    test_seen_loc = matcontent['test_seen_loc'].squeeze() - 1\n",
    "    test_unseen_loc = matcontent['test_unseen_loc'].squeeze() - 1\n",
    "\n",
    "    attribute = matcontent['att'].T    #转置 50x85 每行是整个属性向量\n",
    "\n",
    "    x = feature[trainval_loc]                      # train_features trainval里面是图片的编号 begin with 0 ，19832个\n",
    "    #print(x,x.shape)\n",
    "    train_label = label[trainval_loc].astype(int)  # train_label  int类型没变 每个图片的lable 19832个\n",
    "    #print(train_label,train_label.dtype)\n",
    "    att = attribute[train_label]                   # train attributes 每个图片的属性 19832个\n",
    "    #print(att,att.shape)\n",
    "    \n",
    "\n",
    "    x_test = feature[test_unseen_loc]                   # test_feature 5685个\n",
    "    test_label = label[test_unseen_loc].astype(int)     # test_label   5685个\n",
    "    #print(x_test,x_test.shape)\n",
    "    #print(test_label,test_label.shape)\n",
    "    \n",
    "    x_test_seen = feature[test_seen_loc]                #test_seen_feature 4958个\n",
    "    test_label_seen = label[test_seen_loc].astype(int)  # test_seen_label  4958个\n",
    "    #print(x_test_seen,x_test_seen.shape)\n",
    "    #print(test_label_seen,test_label_seen.shape)\n",
    "    \n",
    "    test_id = np.unique(test_label)                     # test_id  10个类 ，unique去重\n",
    "    att_pro = attribute[test_id]                        # test_attribute 每一类的属性向量 10x85\n",
    "    #print(test_id,test_id.shape)\n",
    "    #print(att_pro,att_pro.shape)\n",
    "\n",
    "    \n",
    "\n",
    "    # train set\n",
    "    train_features = torch.from_numpy(x)   #np-->tensor\n",
    "    #print(train_features.type())\n",
    "    #print(train_features.shape)\n",
    "    sample_attributes=[]\n",
    "    train_label = torch.from_numpy(train_label).unsqueeze(1) #每张图片的属性转化 ，unsqueeze(1)就是插入到第一维度 AxB维-->Ax1xB\n",
    "    #print(train_features)\n",
    "    print(train_features.shape)\n",
    "    print(train_label.shape)\n",
    "\n",
    "    # attributes\n",
    "    all_attributes = np.array(attribute)  #所有50类属性转变为numpy数组???属性向量仍然用的numpy类型 没有转化为pytorch\n",
    "    #print(all_attributes)\n",
    "    \n",
    "    #print('-'*50)\n",
    "    attributes = torch.from_numpy(attribute) \n",
    "    #print(attribute)\n",
    "    # test set\n",
    "\n",
    "    test_features = torch.from_numpy(x_test)\n",
    "    #print(test_features.shape)\n",
    "\n",
    "    test_label = torch.from_numpy(test_label).unsqueeze(1)\n",
    "    #print(test_label.shape)\n",
    "\n",
    "    testclasses_id = np.array(test_id)\n",
    "    #print(testclasses_id.shape)\n",
    "\n",
    "    test_attributes = torch.from_numpy(att_pro).float()\n",
    "    #print(test_attributes.shape)\n",
    "\n",
    "\n",
    "    test_seen_features = torch.from_numpy(x_test_seen)\n",
    "    #print(test_seen_features.shape)\n",
    "\n",
    "    test_seen_label = torch.from_numpy(test_label_seen)\n",
    "\n",
    "    train_data = TensorDataset(train_features,train_label)\n",
    "    #################here need new code to make triplet data#####################\n",
    "    print('-'*100)\n",
    "######################################################################\n",
    "# init network\n",
    "    print(\"init networks\")\n",
    "    attribute_network = AttributeNetwork(85,1024,2048)  #85d属性 1024隐藏层 2048输出 85d到2048d\n",
    "    \n",
    "    #relation_network = RelationNetwork(4096,400)        #4096输入 2048d+2048d 400隐藏层\n",
    "    triplet_network = TripletNetwork(attribute_network)  #metric learning\n",
    "    \n",
    "    attribute_network.cuda(GPU)                 #gpu train net\n",
    "    \n",
    "    triplet_network.cuda(GPU)\n",
    "    \n",
    "    attribute_network_optim = torch.optim.Adam(attribute_network.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
    "    #优化器adam 学习率 正则1e-5\n",
    "\n",
    "    attribute_network_scheduler = StepLR(attribute_network_optim, step_size=200000, gamma=0.5)\n",
    "    #学习率每200k步 乘0.5\n",
    "    triplet_network_optim = torch.optim.Adam(relation_network.parameters(), lr=LEARNING_RATE)\n",
    "    #\n",
    "    triplet_network_scheduler = StepLR(relation_network_optim, step_size=200000, gamma=0.5)\n",
    "    #\n",
    "    print('-'*100)\n",
    "############################################################\n",
    "    print(\"training...\")\n",
    "    last_accuracy = 0.0\n",
    "    for episode in range(EPISODE):\n",
    "        attribute_network_scheduler.step(episode)\n",
    "        triplet_network_scheduler.step(episode)\n",
    "\n",
    "        train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "        batch_features, batch_labels = train_loader.__iter__().next()\n",
    "        \n",
    "        sample_labels = []                   #样本标签 list型\n",
    "        for label in batch_labels.numpy():   #tensor--->numpy\n",
    "            if label not in sample_labels:\n",
    "                sample_labels.append(label)   #没出现的标签加入样本标签的集合\n",
    "        #pdb.set_trace()\n",
    "\n",
    "        sample_attributes = torch.Tensor([all_attributes[i] for i in sample_labels]).squeeze(1)  #batch样本的属性 K x 85\n",
    "        class_num = sample_attributes.shape[0]   #第一维度的长度\n",
    "        #print(sample_attributes.shape)\n",
    "        \n",
    "        batch_features = Variable(batch_features).cuda(GPU).float()  # 32*1024\n",
    "        sample_features = attribute_network(Variable(sample_attributes).cuda(GPU)) #k*312\n",
    "\n",
    "#################################################################################################################\n",
    "#################################################################################################################\n",
    "###################################################################################################################\n",
    "###################################################################################################################\n",
    "###################################################################################################################\n",
    "\n",
    "        sample_features_ext = sample_features.unsqueeze(0).repeat(BATCH_SIZE,1,1)\n",
    "        batch_features_ext = batch_features.unsqueeze(0).repeat(class_num,1,1)\n",
    "        batch_features_ext = torch.transpose(batch_features_ext,0,1)\n",
    "\n",
    "        #print(sample_features_ext)\n",
    "        #print(batch_features_ext)\n",
    "        relation_pairs = torch.cat((sample_features_ext,batch_features_ext),2).view(-1,4096)\n",
    "        #pdb.set_trace()\n",
    "        relations = relation_network(relation_pairs).view(-1,class_num)\n",
    "        #print(relations)\n",
    "\n",
    "        # re-build batch_labels according to sample_labels\n",
    "        sample_labels = np.array(sample_labels)\n",
    "        re_batch_labels = []\n",
    "        for label in batch_labels.numpy():\n",
    "            index = np.argwhere(sample_labels == label)\n",
    "            re_batch_labels.append(index[0][0])\n",
    "        re_batch_labels = torch.LongTensor(re_batch_labels)\n",
    "        # pdb.set_trace()\n",
    "\n",
    "\n",
    "        # loss\n",
    "        mse = nn.MSELoss().cuda(GPU)\n",
    "        one_hot_labels = Variable(torch.zeros(BATCH_SIZE, class_num).scatter_(1, re_batch_labels.view(-1,1), 1)).cuda(GPU)\n",
    "        loss = mse(relations, one_hot_labels)\n",
    "        # pdb.set_trace()\n",
    "\n",
    "        # update\n",
    "        attribute_network.zero_grad()\n",
    "        relation_network.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        attribute_network_optim.step()\n",
    "        relation_network_optim.step()\n",
    "\n",
    "        if (episode+1)%100 == 0:\n",
    "                print(\"episode:\", episode+1, \"loss\", loss.data)\n",
    "\n",
    "        if (episode+1)%2000 == 0:\n",
    "            # test\n",
    "            print(\"Testing...\")\n",
    "\n",
    "            def compute_accuracy(test_features, test_label, test_id, test_attributes):\n",
    "\n",
    "                test_data = TensorDataset(test_features, test_label)\n",
    "                test_batch = 32\n",
    "                test_loader = DataLoader(test_data, batch_size=test_batch, shuffle=False)\n",
    "                total_rewards = 0\n",
    "                # fetch attributes\n",
    "                # pdb.set_trace()\n",
    "\n",
    "                sample_labels = test_id\n",
    "                sample_attributes = test_attributes\n",
    "                class_num = sample_attributes.shape[0]\n",
    "                test_size = test_features.shape[0]\n",
    "\n",
    "                print(\"class num:\", class_num)\n",
    "\n",
    "                for batch_features,batch_labels in test_loader:\n",
    "\n",
    "                    batch_size = batch_labels.shape[0]\n",
    "\n",
    "                    batch_features = Variable(batch_features).cuda(GPU).float()  # 32*1024\n",
    "                    sample_features = attribute_network(Variable(sample_attributes).cuda(GPU).float())\n",
    "\n",
    "                    sample_features_ext = sample_features.unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "                    batch_features_ext = batch_features.unsqueeze(0).repeat(class_num, 1, 1)\n",
    "                    batch_features_ext = torch.transpose(batch_features_ext, 0, 1)\n",
    "\n",
    "                    relation_pairs = torch.cat((sample_features_ext, batch_features_ext), 2).view(-1, 4096)\n",
    "                    relations = relation_network(relation_pairs).view(-1, class_num)\n",
    "\n",
    "                    # re-build batch_labels according to sample_labels\n",
    "\n",
    "                    re_batch_labels = []\n",
    "                    for label in batch_labels.numpy():\n",
    "                        index = np.argwhere(sample_labels == label)\n",
    "                        re_batch_labels.append(index[0][0])\n",
    "                    re_batch_labels = torch.cuda.LongTensor(re_batch_labels)\n",
    "                    # pdb.set_trace()\n",
    "\n",
    "\n",
    "                    _,predict_labels = torch.max(relations.data, 1)\n",
    "\n",
    "                    rewards = [1 if predict_labels[j] == re_batch_labels[j] else 0 for j in range(batch_size)]\n",
    "                    total_rewards += np.sum(rewards)\n",
    "                test_accuracy = total_rewards/1.0/test_size\n",
    "\n",
    "                return test_accuracy\n",
    "\n",
    "            zsl_accuracy = compute_accuracy(test_features, test_label, test_id, test_attributes)\n",
    "            gzsl_unseen_accuracy = compute_accuracy(test_features, test_label, np.arange(50), attributes)\n",
    "            gzsl_seen_accuracy = compute_accuracy(test_seen_features, test_seen_label, np.arange(50), attributes)\n",
    "\n",
    "            H = 2 * gzsl_seen_accuracy * gzsl_unseen_accuracy / (gzsl_unseen_accuracy + gzsl_seen_accuracy)\n",
    "\n",
    "            print('zsl:', zsl_accuracy)\n",
    "            print('gzsl: seen=%.4f, unseen=%.4f, h=%.4f' % (gzsl_seen_accuracy, gzsl_unseen_accuracy, H))\n",
    "\n",
    "\n",
    "            if zsl_accuracy > last_accuracy:\n",
    "\n",
    "                # save networks\n",
    "                torch.save(attribute_network.state_dict(),\"./models/zsl_awa1_attribute_network_v33.pkl\")\n",
    "                torch.save(relation_network.state_dict(),\"./models/zsl_awa1_relation_network_v33.pkl\")\n",
    "\n",
    "                print(\"save networks for episode:\",episode)\n",
    "\n",
    "                last_accuracy = zsl_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-29T22:31:58.468492Z",
     "start_time": "2018-05-29T22:31:06.736453Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
